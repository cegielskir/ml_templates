{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eefdca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch #pytorch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e1815f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4561ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_scaled[:, :-1])\n",
    "y = df_scaled[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9ad8b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:-24]\n",
    "y = y[:-24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fc1d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_len):\n",
    "    X_seq = []\n",
    "    Y_seq = []\n",
    "    for i in range(0, len(X) -1 - seq_len):\n",
    "        x_seq_row = []\n",
    "        for j in range(i, i + seq_len):\n",
    "            x_seq_row.append(X[j])\n",
    "        X_seq.append(x_seq_row)\n",
    "        Y_seq.append(y[j + 1])\n",
    "    return X_seq, Y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "30ccfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x=torch.tensor(X).float().to(DEVICE)\n",
    "        self.y=torch.tensor(y).float().to(DEVICE)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a935e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq, Y_seq = create_sequences(X, y, 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d0da8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, Y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9704e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(X_train, y_train)\n",
    "val_data = Data(X_val, y_val)\n",
    "test_data = Data(X_test, y_test)\n",
    "\n",
    "train_loader=DataLoader(dataset=train_data,batch_size=256)\n",
    "val_loader=DataLoader(dataset=val_data,batch_size=256)\n",
    "test_loader=DataLoader(dataset=test_data,batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2914ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc =  nn.Linear(hidden_size, num_classes) #fully connected 1\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(DEVICE) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(DEVICE) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        out = hn[-1]\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3549c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 5 #number of features\n",
    "hidden_size = 72 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "seq_len = 24\n",
    "\n",
    "num_classes = 2 #number of output classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "27388651",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers, seq_len).to(DEVICE) #our lstm class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "34a84da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d1f07fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 79.58773 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 10, loss: 79.65410 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 20, loss: 79.62092 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 30, loss: 79.58837 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 40, loss: 79.56387 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 50, loss: 79.52663 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 60, loss: 79.52381 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 70, loss: 79.42340 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 80, loss: 79.36748 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 90, loss: 79.35567 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 100, loss: 79.28214 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 110, loss: 79.29112 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 120, loss: 79.07568 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 130, loss: 78.97124 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 140, loss: 78.87089 : Acc = 76.742 : T Acc = 78.103\n",
      "Epoch: 150, loss: 78.91138 : Acc = 76.734 : T Acc = 78.103\n",
      "Epoch: 160, loss: 78.62866 : Acc = 76.777 : T Acc = 78.088\n",
      "Epoch: 170, loss: 78.50249 : Acc = 76.740 : T Acc = 78.103\n",
      "Epoch: 180, loss: 78.33810 : Acc = 76.766 : T Acc = 78.103\n",
      "Epoch: 190, loss: 78.05193 : Acc = 76.828 : T Acc = 77.952\n",
      "Epoch: 200, loss: 78.05486 : Acc = 76.812 : T Acc = 77.952\n",
      "Epoch: 210, loss: 77.87050 : Acc = 76.796 : T Acc = 77.816\n",
      "Epoch: 220, loss: 77.65252 : Acc = 76.894 : T Acc = 77.786\n",
      "Epoch: 230, loss: 77.31559 : Acc = 76.934 : T Acc = 77.847\n",
      "Epoch: 240, loss: 77.48344 : Acc = 76.926 : T Acc = 77.801\n",
      "Epoch: 250, loss: 76.89029 : Acc = 77.017 : T Acc = 77.711\n",
      "Epoch: 260, loss: 77.17695 : Acc = 76.958 : T Acc = 77.696\n",
      "Epoch: 270, loss: 76.63554 : Acc = 77.105 : T Acc = 77.560\n",
      "Epoch: 280, loss: 76.24757 : Acc = 77.174 : T Acc = 77.680\n",
      "Epoch: 290, loss: 76.12959 : Acc = 77.153 : T Acc = 77.273\n",
      "Epoch: 300, loss: 75.69742 : Acc = 77.233 : T Acc = 77.318\n",
      "Epoch: 310, loss: 75.12551 : Acc = 77.313 : T Acc = 77.001\n",
      "Epoch: 320, loss: 75.74489 : Acc = 77.185 : T Acc = 77.137\n",
      "Epoch: 330, loss: 74.28357 : Acc = 77.446 : T Acc = 77.076\n",
      "Epoch: 340, loss: 74.13746 : Acc = 77.422 : T Acc = 76.910\n",
      "Epoch: 350, loss: 73.92002 : Acc = 77.531 : T Acc = 76.956\n",
      "Epoch: 360, loss: 73.69642 : Acc = 77.566 : T Acc = 76.865\n",
      "Epoch: 370, loss: 72.79495 : Acc = 77.723 : T Acc = 76.865\n",
      "Epoch: 380, loss: 73.15538 : Acc = 77.678 : T Acc = 75.959\n",
      "Epoch: 390, loss: 72.42754 : Acc = 77.809 : T Acc = 75.914\n",
      "Epoch: 400, loss: 72.25141 : Acc = 77.867 : T Acc = 76.321\n",
      "Epoch: 410, loss: 71.75257 : Acc = 78.110 : T Acc = 75.430\n",
      "Epoch: 420, loss: 71.82676 : Acc = 78.112 : T Acc = 75.717\n",
      "Epoch: 430, loss: 71.93221 : Acc = 78.107 : T Acc = 75.989\n",
      "Epoch: 440, loss: 71.98586 : Acc = 78.024 : T Acc = 75.929\n",
      "Epoch: 450, loss: 71.20438 : Acc = 78.227 : T Acc = 75.687\n",
      "Epoch: 460, loss: 71.99304 : Acc = 78.038 : T Acc = 76.155\n",
      "Epoch: 470, loss: 70.55993 : Acc = 78.475 : T Acc = 75.340\n",
      "Epoch: 480, loss: 70.63747 : Acc = 78.422 : T Acc = 75.053\n",
      "Epoch: 490, loss: 69.74052 : Acc = 78.648 : T Acc = 74.570\n",
      "Epoch: 500, loss: 69.18686 : Acc = 78.696 : T Acc = 74.132\n",
      "Epoch: 510, loss: 68.39033 : Acc = 78.957 : T Acc = 73.180\n",
      "Epoch: 520, loss: 67.77030 : Acc = 78.987 : T Acc = 73.648\n",
      "Epoch: 530, loss: 70.30735 : Acc = 78.304 : T Acc = 74.207\n",
      "Epoch: 540, loss: 69.85519 : Acc = 78.515 : T Acc = 75.023\n",
      "Epoch: 550, loss: 68.50758 : Acc = 78.824 : T Acc = 73.799\n",
      "Epoch: 560, loss: 67.81485 : Acc = 79.157 : T Acc = 73.724\n",
      "Epoch: 570, loss: 67.28046 : Acc = 79.224 : T Acc = 74.434\n",
      "Epoch: 580, loss: 67.00184 : Acc = 79.251 : T Acc = 72.924\n",
      "Epoch: 590, loss: 67.86545 : Acc = 78.968 : T Acc = 73.482\n",
      "Epoch: 600, loss: 67.60714 : Acc = 79.219 : T Acc = 73.256\n",
      "Epoch: 610, loss: 66.21882 : Acc = 79.504 : T Acc = 73.362\n",
      "Epoch: 620, loss: 66.57005 : Acc = 79.450 : T Acc = 74.283\n",
      "Epoch: 630, loss: 68.70568 : Acc = 78.923 : T Acc = 74.373\n",
      "Epoch: 640, loss: 66.11053 : Acc = 79.653 : T Acc = 74.207\n",
      "Epoch: 650, loss: 66.23286 : Acc = 79.482 : T Acc = 73.377\n",
      "Epoch: 660, loss: 65.80264 : Acc = 79.872 : T Acc = 73.664\n",
      "Epoch: 670, loss: 65.67476 : Acc = 79.813 : T Acc = 72.893\n",
      "Epoch: 680, loss: 64.56788 : Acc = 80.210 : T Acc = 73.920\n",
      "Epoch: 690, loss: 63.58696 : Acc = 80.381 : T Acc = 72.893\n",
      "Epoch: 700, loss: 62.67829 : Acc = 80.639 : T Acc = 72.863\n",
      "Epoch: 710, loss: 62.19727 : Acc = 80.866 : T Acc = 70.900\n",
      "Epoch: 720, loss: 62.91632 : Acc = 80.581 : T Acc = 72.002\n",
      "Epoch: 730, loss: 60.76752 : Acc = 81.196 : T Acc = 71.293\n",
      "Epoch: 740, loss: 59.75179 : Acc = 81.559 : T Acc = 71.534\n",
      "Epoch: 750, loss: 59.39794 : Acc = 81.647 : T Acc = 71.232\n",
      "Epoch: 760, loss: 59.30050 : Acc = 81.775 : T Acc = 72.757\n",
      "Epoch: 770, loss: 62.57430 : Acc = 80.828 : T Acc = 72.561\n",
      "Epoch: 780, loss: 58.43046 : Acc = 82.033 : T Acc = 72.244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25101/2931859284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_arr = []\n",
    "train_acc_arr = []\n",
    "val_loss_arr = []\n",
    "val_acc_arr = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    train_loss = 0\n",
    "    lstm.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = lstm.forward(X_batch) #forward pass\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        loss = criterion(outputs, y_batch.long())\n",
    "\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        total_correct += (predictions == y_batch).sum().item()\n",
    "        total_samples += y_batch.size(0)\n",
    "        train_loss += loss.item()\n",
    "    train_loss_arr.append(train_loss)\n",
    "     \n",
    "    val_total_correct = 0\n",
    "    val_total_samples = 0\n",
    "    val_loss = 0\n",
    "    lstm.eval()\n",
    "    for X_val_batch, y_val_batch in val_loader:\n",
    "        outputs = lstm.forward(X_val_batch) #forward pass\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, y_val_batch.long())\n",
    "\n",
    "\n",
    "        val_total_correct += (predictions == y_val_batch).sum().item()\n",
    "        val_total_samples += y_val_batch.size(0)\n",
    "        val_loss += loss.item()\n",
    "    val_loss_arr.append(val_loss)\n",
    "    \n",
    "    train_accuracy = 100 * total_correct / total_samples\n",
    "    train_acc_arr.append(train_accuracy)   \n",
    "    \n",
    "    val_accuracy = 100 * val_total_correct / val_total_samples\n",
    "    val_acc_arr.append(val_accuracy)   \n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, train_loss) + ' : Acc = %1.3f' % train_accuracy + ' : T Acc = %1.3f' % val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([0.1])).to('cuda')\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5) \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm = lstm.to(device)\n",
    "\n",
    "y_train_tensors = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train_tensors = X_train_tensors.to(device)\n",
    "y_train_tensors = y_train_tensors.to(device)\n",
    "batch_size = 256\n",
    "batches_per_epoch = int(len(X_train_tensors) / batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    lstm.train()\n",
    "    for i in range(batches_per_epoch):\n",
    "        start = i * batch_size\n",
    "        # take a batch\n",
    "        Xbatch = X_train_tensors[start:start+batch_size]\n",
    "        ybatch = y_train_tensors[start:start+batch_size]\n",
    "        outputs = lstm.forward(Xbatch) #forward pass\n",
    "#         predictions = torch.argmax(outputs, dim=1)\n",
    "#         predictions = torch.round(outputs)\n",
    "        predictions = (outputs > 0.0).float()\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = criterion(outputs, ybatch.float())\n",
    "\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "#         print(predictions)\n",
    "#         print(ybatch)\n",
    "#         print('====================')\n",
    "        total_correct += (predictions == ybatch).sum().item()\n",
    "#         print((predictions == ybatch))\n",
    "        total_samples += ybatch.size(0)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        false_correct = 0\n",
    "        positive_correct = 0\n",
    "\n",
    "        lstm.eval()\n",
    "        X_test_tensors = X_test_tensors.to(device)\n",
    "        y_test_tensors = y_test_tensors.to(device)\n",
    "        outputs = lstm.forward(X_test_tensors) #forward pass\n",
    "        # predictions = torch.argmax(outputs, dim=1)\n",
    "        predictions = (outputs > 0.0).float()\n",
    "#         print(predictions)\n",
    "#         print(y_test_tensors[y_test_tensors ==1])\n",
    "        total_correct += (predictions == y_test_tensors).sum().item()\n",
    "        false_correct += (predictions[y_test_tensors ==0] == y_test_tensors[y_test_tensors ==0]).sum().item()\n",
    "        positive_correct += (predictions[y_test_tensors ==1] == y_test_tensors[y_test_tensors ==1]).sum().item()\n",
    "        positive_samples = torch.count_nonzero(y_test_tensors)\n",
    "        total_samples = y_test_tensors.size(0)\n",
    "        negative_samples = total_samples -positive_samples\n",
    "\n",
    "        test_accuracy = 100 * total_correct / total_samples\n",
    "        p_test_accuracy = 100 * positive_correct / positive_samples\n",
    "        n_test_accuracy = 100 * false_correct / negative_samples\n",
    "\n",
    "        print(\"Ech: %d, ls: %1.5f\" % (epoch, loss.item()) + ' : Acc = %2.2f' % accuracy + ' : T Acc = %2.2f' % test_accuracy + ' : TP Acc = %2.2f' % p_test_accuracy + ' : TN Acc = %2.4f' % n_test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
